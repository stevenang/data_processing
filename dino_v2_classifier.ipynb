{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-30T23:39:28.582663Z",
     "start_time": "2025-04-30T23:39:27.365053Z"
    }
   },
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import timm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.transform import resize"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenang/DataspellProjects/DataProcessing/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T23:39:28.588593Z",
     "start_time": "2025-04-30T23:39:28.584933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.target_size = 224  # Must match the model's expected input size\n",
    "\n",
    "    def normalize_slice(self, slice_data):\n",
    "        # Z-score normalization often works better for medical images\n",
    "        mean = np.mean(slice_data)\n",
    "        std = np.std(slice_data)\n",
    "        return (slice_data - mean) / (std + 1e-8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load NIfTI file\n",
    "        img_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            # Load and preprocess NIfTI image\n",
    "            nifti_img = nib.load(img_path)\n",
    "            img_data = nifti_img.get_fdata()\n",
    "\n",
    "            # Process to get 3-channel 2D representation\n",
    "            if len(img_data.shape) == 3:\n",
    "                x_mid = img_data.shape[0] // 2\n",
    "                y_mid = img_data.shape[1] // 2\n",
    "                z_mid = img_data.shape[2] // 2\n",
    "\n",
    "                # Extract slices\n",
    "                slice1 = img_data[x_mid, :, :]\n",
    "                slice2 = img_data[:, y_mid, :]\n",
    "                slice3 = img_data[:, :, z_mid]\n",
    "\n",
    "                # Apply z-score normalization instead of min-max\n",
    "                slice1 = self.normalize_slice(slice1)\n",
    "                slice2 = self.normalize_slice(slice2)\n",
    "                slice3 = self.normalize_slice(slice3)\n",
    "\n",
    "                # Resize to exactly 224x224 for ViT models\n",
    "                slice1_resized = resize(slice1, (self.target_size, self.target_size), anti_aliasing=True)\n",
    "                slice2_resized = resize(slice2, (self.target_size, self.target_size), anti_aliasing=True)\n",
    "                slice3_resized = resize(slice3, (self.target_size, self.target_size), anti_aliasing=True)\n",
    "\n",
    "                # Stack along new channel dimension\n",
    "                img_array = np.stack([slice1_resized, slice2_resized, slice3_resized], axis=0)\n",
    "\n",
    "                # Convert to tensor\n",
    "                img_tensor = torch.from_numpy(img_array).float()\n",
    "\n",
    "                return img_tensor, torch.tensor(label, dtype=torch.long)\n",
    "            else:\n",
    "                raise ValueError(f\"Expected 3D NIfTI data, got shape {img_data.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            # Return a placeholder tensor with correct size\n",
    "            img_tensor = torch.zeros((3, self.target_size, self.target_size), dtype=torch.float32)\n",
    "            return img_tensor, torch.tensor(label, dtype=torch.long)"
   ],
   "id": "75e4af2e8d710cd1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T23:39:28.637792Z",
     "start_time": "2025-04-30T23:39:28.636148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load DINOv2 model\n",
    "def load_dinov2_model(variant=\"vitb14\"):\n",
    "    \"\"\"Load DINOv2 model for feature extraction\"\"\"\n",
    "    model = torch.hub.load('facebookresearch/dinov2', f'dinov2_{variant}')\n",
    "    return model"
   ],
   "id": "3a1971aa8c9b1cf3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T23:39:28.644105Z",
     "start_time": "2025-04-30T23:39:28.641110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ADHDClassifier(nn.Module):\n",
    "    def __init__(self, backbone=\"resnet18\", freeze_backbone=True):\n",
    "        super(ADHDClassifier, self).__init__()\n",
    "        self.backbone_name = backbone\n",
    "\n",
    "        # Map custom backbone names to valid timm model names\n",
    "        if backbone == \"vitb14\":\n",
    "            # Use base model with patch size 16 instead, which has pretrained weights\n",
    "            timm_model_name = \"vit_base_patch16_224\"\n",
    "            use_pretrained = True\n",
    "        else:\n",
    "            # For other backbone names, try to use them directly\n",
    "            timm_model_name = backbone\n",
    "            use_pretrained = True\n",
    "\n",
    "        try:\n",
    "            # Create the backbone using the mapped name\n",
    "            self.backbone = timm.create_model(timm_model_name, pretrained=use_pretrained)\n",
    "\n",
    "            # Replace the head of the model\n",
    "            embed_dim = self.backbone.head.in_features\n",
    "            self.backbone.head = nn.Identity()  # Remove classification head\n",
    "\n",
    "            # Create a new classifier head\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(embed_dim, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, 2)  # Binary classification: ADHD vs Control\n",
    "            )\n",
    "\n",
    "            # Freeze backbone if requested and using pretrained\n",
    "            if freeze_backbone and use_pretrained:\n",
    "                for param in self.backbone.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating model with {timm_model_name}: {e}\")\n",
    "            print(\"Trying with vit_base_patch16_224 without pretrained weights...\")\n",
    "\n",
    "            # Fallback to vit_base_patch16_224 without pretrained weights\n",
    "            self.backbone = timm.create_model('vit_base_patch16_224', pretrained=False)\n",
    "            embed_dim = self.backbone.head.in_features\n",
    "            self.backbone.head = nn.Identity()\n",
    "\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(embed_dim, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, 2)\n",
    "            )\n",
    "\n",
    "            # Don't freeze if using random initialization\n",
    "            if freeze_backbone:\n",
    "                print(\"Note: Not freezing backbone since we're using random initialization\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add this resize operation before passing to backbone\n",
    "        if x.shape[-1] != 224 or x.shape[-2] != 224:\n",
    "            #print(f\"Resizing input from {x.shape[-2]}x{x.shape[-1]} to 224x224\")\n",
    "            x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(features)\n",
    "\n",
    "        return logits"
   ],
   "id": "9f4ae60648e0fbea",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T23:39:28.650589Z",
     "start_time": "2025-04-30T23:39:28.647042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_adhd_classifier(model, train_loader, val_loader, epochs=20):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Add the scheduler here\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Calculate average training loss\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Update the scheduler based on validation accuracy\n",
    "        scheduler.step(val_accuracy)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            best_model = model.state_dict().copy()\n",
    "\n",
    "    # Load the best model\n",
    "    if best_model:\n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "    return model"
   ],
   "id": "a6c8049f5c6c5005",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T23:39:28.655197Z",
     "start_time": "2025-04-30T23:39:28.653560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_collate(batch):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for image, label in batch:\n",
    "        # Ensure image is exactly 256x256\n",
    "        if image.shape[1] != 256 or image.shape[2] != 256:\n",
    "            # Resize to 256x256 using F.interpolate\n",
    "            image = F.interpolate(image.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Stack all images and labels\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return images, labels"
   ],
   "id": "b9708dad20a20751",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T23:39:28.659869Z",
     "start_time": "2025-04-30T23:39:28.658036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Transform\n",
    "#transform = T.Compose([\n",
    "    # Convert to PIL image first (required for many transforms)\n",
    "#    T.ToPILImage(),\n",
    "    # Resize to exactly 256x256\n",
    "#    T.Resize((256, 256)),\n",
    "    # Convert back to tensor\n",
    "#    T.ToTensor(),\n",
    "#    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#])\n",
    "# Define transform without resize (we'll handle it in the collate function)\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ],
   "id": "e75a53ee5e2131cc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T23:39:28.666208Z",
     "start_time": "2025-04-30T23:39:28.663408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "adhd_image_path = \"image_data/adhd\"\n",
    "control_image_path = \"image_data/control\"\n",
    "\n",
    "adhd_files = [os.path.join(adhd_image_path, file_path) for file_path in os.listdir(adhd_image_path) if file_path.endswith(\".nii\") or file_path.endswith(\".nii.gz\")]\n",
    "control_files = [os.path.join(adhd_image_path, file_path) for file_path in os.listdir(adhd_image_path) if file_path.endswith(\".nii\") or file_path.endswith(\".nii.gz\")]\n",
    "\n",
    "all_files = adhd_files + control_files\n",
    "labels = [1] * len(adhd_files) + [0] * len(control_files)\n",
    "\n",
    "# Split dataset\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(\n",
    "    all_files, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")"
   ],
   "id": "7e2b7d48e3e9cba8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T23:39:28.670984Z",
     "start_time": "2025-04-30T23:39:28.669633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create datasets and data loaders\n",
    "train_dataset = MRIDataset(train_files, train_labels)\n",
    "test_dataset = MRIDataset(test_files, test_labels)"
   ],
   "id": "f87fd313d3d840b0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T23:39:28.675910Z",
     "start_time": "2025-04-30T23:39:28.674311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ],
   "id": "fb66c76c5e9b521c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T23:39:29.235870Z",
     "start_time": "2025-04-30T23:39:28.679219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify the size of a batch\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch shape: {images.shape}\")  # Should be [batch_size, 3, 224, 224]\n",
    "    break"
   ],
   "id": "195f8354b19dbd8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([16, 3, 224, 224])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T23:43:13.903333Z",
     "start_time": "2025-04-30T23:39:29.243025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train model\n",
    "model = ADHDClassifier(backbone=\"vitb14\", freeze_backbone=True)\n",
    "trained_model = train_adhd_classifier(model, train_loader, test_loader, epochs=20)"
   ],
   "id": "b20609bb3445b8cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 1.2772\n",
      "Validation Loss: 1.8824, Accuracy: 48.65%\n",
      "Epoch 2/20, Train Loss: 1.2883\n",
      "Validation Loss: 0.7749, Accuracy: 51.35%\n",
      "Epoch 3/20, Train Loss: 0.7767\n",
      "Validation Loss: 0.6952, Accuracy: 51.35%\n",
      "Epoch 4/20, Train Loss: 0.6871\n",
      "Validation Loss: 0.7303, Accuracy: 45.95%\n",
      "Epoch 5/20, Train Loss: 0.6924\n",
      "Validation Loss: 0.7005, Accuracy: 45.95%\n",
      "Epoch 6/20, Train Loss: 0.6951\n",
      "Validation Loss: 0.7156, Accuracy: 29.73%\n",
      "Epoch 7/20, Train Loss: 0.6978\n",
      "Validation Loss: 0.7126, Accuracy: 29.73%\n",
      "Epoch 8/20, Train Loss: 0.6890\n",
      "Validation Loss: 0.7179, Accuracy: 21.62%\n",
      "Epoch 9/20, Train Loss: 0.6935\n",
      "Validation Loss: 0.7205, Accuracy: 29.73%\n",
      "Epoch 10/20, Train Loss: 0.6891\n",
      "Validation Loss: 0.7249, Accuracy: 21.62%\n",
      "Epoch 11/20, Train Loss: 0.6888\n",
      "Validation Loss: 0.7241, Accuracy: 18.92%\n",
      "Epoch 12/20, Train Loss: 0.6858\n",
      "Validation Loss: 0.7344, Accuracy: 35.14%\n",
      "Epoch 13/20, Train Loss: 0.6872\n",
      "Validation Loss: 0.7239, Accuracy: 32.43%\n",
      "Epoch 14/20, Train Loss: 0.6853\n",
      "Validation Loss: 0.7360, Accuracy: 29.73%\n",
      "Epoch 15/20, Train Loss: 0.6789\n",
      "Validation Loss: 0.7308, Accuracy: 24.32%\n",
      "Epoch 16/20, Train Loss: 0.6907\n",
      "Validation Loss: 0.7295, Accuracy: 29.73%\n",
      "Epoch 17/20, Train Loss: 0.6886\n",
      "Validation Loss: 0.7300, Accuracy: 24.32%\n",
      "Epoch 18/20, Train Loss: 0.6871\n",
      "Validation Loss: 0.7226, Accuracy: 29.73%\n",
      "Epoch 19/20, Train Loss: 0.6861\n",
      "Validation Loss: 0.7366, Accuracy: 27.03%\n",
      "Epoch 20/20, Train Loss: 0.6878\n",
      "Validation Loss: 0.7320, Accuracy: 27.03%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T23:43:16.194522Z",
     "start_time": "2025-04-30T23:43:13.917458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = trained_model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Print classification matrix\n",
    "    print(classification_report(all_labels, all_preds, target_names=[\"Control\", \"ADHD\"]))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))"
   ],
   "id": "12d877fc04522aff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.17      0.11      0.13        19\n",
      "        ADHD       0.32      0.44      0.37        18\n",
      "\n",
      "    accuracy                           0.27        37\n",
      "   macro avg       0.24      0.27      0.25        37\n",
      "weighted avg       0.24      0.27      0.25        37\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 2 17]\n",
      " [10  8]]\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
